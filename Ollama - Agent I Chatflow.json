{
  "nodes": [
    {
      "id": "chatOllama_0",
      "position": {
        "x": 919.3379683087619,
        "y": 568.1591236301284
      },
      "type": "customNode",
      "data": {
        "id": "chatOllama_0",
        "label": "ChatOllama",
        "version": 4,
        "name": "chatOllama",
        "type": "ChatOllama",
        "baseClasses": [
          "ChatOllama",
          "ChatOllama",
          "BaseChatModel",
          "BaseLanguageModel",
          "Runnable"
        ],
        "category": "Chat Models",
        "description": "Chat completion using open-source LLM on Ollama",
        "inputParams": [
          {
            "label": "Base URL",
            "name": "baseUrl",
            "type": "string",
            "default": "http://localhost:11434",
            "id": "chatOllama_0-input-baseUrl-string"
          },
          {
            "label": "Model Name",
            "name": "modelName",
            "type": "string",
            "placeholder": "llama2",
            "id": "chatOllama_0-input-modelName-string"
          },
          {
            "label": "Temperature",
            "name": "temperature",
            "type": "number",
            "description": "The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 0.1,
            "default": 0.9,
            "optional": true,
            "id": "chatOllama_0-input-temperature-number"
          },
          {
            "label": "Allow Image Uploads",
            "name": "allowImageUploads",
            "type": "boolean",
            "description": "Allow image uploads for multimodal models. e.g. llama3.2-vision",
            "default": false,
            "optional": true,
            "id": "chatOllama_0-input-allowImageUploads-boolean"
          },
          {
            "label": "JSON Mode",
            "name": "jsonMode",
            "type": "boolean",
            "description": "Coerces model outputs to only return JSON. Specify in the system prompt to return JSON. Ex: Format all responses as JSON object",
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-jsonMode-boolean"
          },
          {
            "label": "Keep Alive",
            "name": "keepAlive",
            "type": "string",
            "description": "How long to keep connection alive. A duration string (such as \"10m\" or \"24h\")",
            "default": "5m",
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-keepAlive-string"
          },
          {
            "label": "Top P",
            "name": "topP",
            "type": "number",
            "description": "Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-topP-number"
          },
          {
            "label": "Top K",
            "name": "topK",
            "type": "number",
            "description": "Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-topK-number"
          },
          {
            "label": "Mirostat",
            "name": "mirostat",
            "type": "number",
            "description": "Enable Mirostat sampling for controlling perplexity. (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-mirostat-number"
          },
          {
            "label": "Mirostat ETA",
            "name": "mirostatEta",
            "type": "number",
            "description": "Influences how quickly the algorithm responds to feedback from the generated text. A lower learning rate will result in slower adjustments, while a higher learning rate will make the algorithm more responsive. (Default: 0.1) Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-mirostatEta-number"
          },
          {
            "label": "Mirostat TAU",
            "name": "mirostatTau",
            "type": "number",
            "description": "Controls the balance between coherence and diversity of the output. A lower value will result in more focused and coherent text. (Default: 5.0) Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-mirostatTau-number"
          },
          {
            "label": "Context Window Size",
            "name": "numCtx",
            "type": "number",
            "description": "Sets the size of the context window used to generate the next token. (Default: 2048) Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-numCtx-number"
          },
          {
            "label": "Number of GPU",
            "name": "numGpu",
            "type": "number",
            "description": "The number of layers to send to the GPU(s). On macOS it defaults to 1 to enable metal support, 0 to disable. Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-numGpu-number"
          },
          {
            "label": "Number of Thread",
            "name": "numThread",
            "type": "number",
            "description": "Sets the number of threads to use during computation. By default, Ollama will detect this for optimal performance. It is recommended to set this value to the number of physical CPU cores your system has (as opposed to the logical number of cores). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-numThread-number"
          },
          {
            "label": "Repeat Last N",
            "name": "repeatLastN",
            "type": "number",
            "description": "Sets how far back for the model to look back to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-repeatLastN-number"
          },
          {
            "label": "Repeat Penalty",
            "name": "repeatPenalty",
            "type": "number",
            "description": "Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-repeatPenalty-number"
          },
          {
            "label": "Stop Sequence",
            "name": "stop",
            "type": "string",
            "rows": 4,
            "placeholder": "AI assistant:",
            "description": "Sets the stop sequences to use. Use comma to seperate different sequences. Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-stop-string"
          },
          {
            "label": "Tail Free Sampling",
            "name": "tfsZ",
            "type": "number",
            "description": "Tail free sampling is used to reduce the impact of less probable tokens from the output. A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting. (Default: 1). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-tfsZ-number"
          }
        ],
        "inputAnchors": [
          {
            "label": "Cache",
            "name": "cache",
            "type": "BaseCache",
            "optional": true,
            "id": "chatOllama_0-input-cache-BaseCache"
          }
        ],
        "inputs": {
          "cache": "",
          "baseUrl": "http://localhost:11434",
          "modelName": "llama3.2",
          "temperature": 0.9,
          "allowImageUploads": "",
          "jsonMode": "",
          "keepAlive": "5m",
          "topP": "",
          "topK": "",
          "mirostat": "",
          "mirostatEta": "",
          "mirostatTau": "",
          "numCtx": "",
          "numGpu": "",
          "numThread": "",
          "repeatLastN": "",
          "repeatPenalty": "",
          "stop": "",
          "tfsZ": ""
        },
        "outputAnchors": [
          {
            "id": "chatOllama_0-output-chatOllama-ChatOllama|ChatOllama|BaseChatModel|BaseLanguageModel|Runnable",
            "name": "chatOllama",
            "label": "ChatOllama",
            "description": "Chat completion using open-source LLM on Ollama",
            "type": "ChatOllama | ChatOllama | BaseChatModel | BaseLanguageModel | Runnable"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 675,
      "selected": false,
      "positionAbsolute": {
        "x": 919.3379683087619,
        "y": 568.1591236301284
      },
      "dragging": false
    },
    {
      "id": "recursiveCharacterTextSplitter_0",
      "position": {
        "x": 65.51793773158204,
        "y": -964.1536215547612
      },
      "type": "customNode",
      "data": {
        "id": "recursiveCharacterTextSplitter_0",
        "label": "Recursive Character Text Splitter",
        "version": 2,
        "name": "recursiveCharacterTextSplitter",
        "type": "RecursiveCharacterTextSplitter",
        "baseClasses": [
          "RecursiveCharacterTextSplitter",
          "TextSplitter",
          "BaseDocumentTransformer",
          "Runnable"
        ],
        "category": "Text Splitters",
        "description": "Split documents recursively by different characters - starting with \"\\n\\n\", then \"\\n\", then \" \"",
        "inputParams": [
          {
            "label": "Chunk Size",
            "name": "chunkSize",
            "type": "number",
            "description": "Number of characters in each chunk. Default is 1000.",
            "default": 1000,
            "optional": true,
            "id": "recursiveCharacterTextSplitter_0-input-chunkSize-number"
          },
          {
            "label": "Chunk Overlap",
            "name": "chunkOverlap",
            "type": "number",
            "description": "Number of characters to overlap between chunks. Default is 200.",
            "default": 200,
            "optional": true,
            "id": "recursiveCharacterTextSplitter_0-input-chunkOverlap-number"
          },
          {
            "label": "Custom Separators",
            "name": "separators",
            "type": "string",
            "rows": 4,
            "description": "Array of custom separators to determine when to split the text, will override the default separators",
            "placeholder": "[\"|\", \"##\", \">\", \"-\"]",
            "additionalParams": true,
            "optional": true,
            "id": "recursiveCharacterTextSplitter_0-input-separators-string"
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "chunkSize": "500",
          "chunkOverlap": "20",
          "separators": ""
        },
        "outputAnchors": [
          {
            "id": "recursiveCharacterTextSplitter_0-output-recursiveCharacterTextSplitter-RecursiveCharacterTextSplitter|TextSplitter|BaseDocumentTransformer|Runnable",
            "name": "recursiveCharacterTextSplitter",
            "label": "RecursiveCharacterTextSplitter",
            "description": "Split documents recursively by different characters - starting with \"\\n\\n\", then \"\\n\", then \" \"",
            "type": "RecursiveCharacterTextSplitter | TextSplitter | BaseDocumentTransformer | Runnable"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 429,
      "selected": false,
      "positionAbsolute": {
        "x": 65.51793773158204,
        "y": -964.1536215547612
      },
      "dragging": false
    },
    {
      "id": "ollamaEmbedding_0",
      "position": {
        "x": 479.5587700231388,
        "y": -422.8471550850383
      },
      "type": "customNode",
      "data": {
        "id": "ollamaEmbedding_0",
        "label": "Ollama Embeddings",
        "version": 1,
        "name": "ollamaEmbedding",
        "type": "OllamaEmbeddings",
        "baseClasses": [
          "OllamaEmbeddings",
          "Embeddings"
        ],
        "category": "Embeddings",
        "description": "Generate embeddings for a given text using open source model on Ollama",
        "inputParams": [
          {
            "label": "Base URL",
            "name": "baseUrl",
            "type": "string",
            "default": "http://localhost:11434",
            "id": "ollamaEmbedding_0-input-baseUrl-string"
          },
          {
            "label": "Model Name",
            "name": "modelName",
            "type": "string",
            "placeholder": "llama2",
            "id": "ollamaEmbedding_0-input-modelName-string"
          },
          {
            "label": "Number of GPU",
            "name": "numGpu",
            "type": "number",
            "description": "The number of layers to send to the GPU(s). On macOS it defaults to 1 to enable metal support, 0 to disable. Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "ollamaEmbedding_0-input-numGpu-number"
          },
          {
            "label": "Number of Thread",
            "name": "numThread",
            "type": "number",
            "description": "Sets the number of threads to use during computation. By default, Ollama will detect this for optimal performance. It is recommended to set this value to the number of physical CPU cores your system has (as opposed to the logical number of cores). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "ollamaEmbedding_0-input-numThread-number"
          },
          {
            "label": "Use MMap",
            "name": "useMMap",
            "type": "boolean",
            "default": true,
            "optional": true,
            "additionalParams": true,
            "id": "ollamaEmbedding_0-input-useMMap-boolean"
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "baseUrl": "http://localhost:11434",
          "modelName": "nomic-embed-text",
          "numGpu": "",
          "numThread": "",
          "useMMap": true
        },
        "outputAnchors": [
          {
            "id": "ollamaEmbedding_0-output-ollamaEmbedding-OllamaEmbeddings|Embeddings",
            "name": "ollamaEmbedding",
            "label": "OllamaEmbeddings",
            "description": "Generate embeddings for a given text using open source model on Ollama",
            "type": "OllamaEmbeddings | Embeddings"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 429,
      "selected": false,
      "positionAbsolute": {
        "x": 479.5587700231388,
        "y": -422.8471550850383
      },
      "dragging": false
    },
    {
      "id": "retrieverTool_0",
      "position": {
        "x": 1299.9525798908048,
        "y": -768.4934911176385
      },
      "type": "customNode",
      "data": {
        "id": "retrieverTool_0",
        "label": "Retriever Tool",
        "version": 3,
        "name": "retrieverTool",
        "type": "RetrieverTool",
        "baseClasses": [
          "RetrieverTool",
          "DynamicTool",
          "Tool",
          "StructuredTool",
          "Runnable"
        ],
        "category": "Tools",
        "description": "Use a retriever as allowed tool for agent",
        "inputParams": [
          {
            "label": "Retriever Name",
            "name": "name",
            "type": "string",
            "placeholder": "search_state_of_union",
            "id": "retrieverTool_0-input-name-string"
          },
          {
            "label": "Retriever Description",
            "name": "description",
            "type": "string",
            "description": "When should agent uses to retrieve documents",
            "rows": 3,
            "placeholder": "Searches and returns documents regarding the state-of-the-union.",
            "id": "retrieverTool_0-input-description-string"
          },
          {
            "label": "Return Source Documents",
            "name": "returnSourceDocuments",
            "type": "boolean",
            "optional": true,
            "id": "retrieverTool_0-input-returnSourceDocuments-boolean"
          },
          {
            "label": "Additional Metadata Filter",
            "name": "retrieverToolMetadataFilter",
            "type": "json",
            "description": "Add additional metadata filter on top of the existing filter from vector store",
            "optional": true,
            "additionalParams": true,
            "hint": {
              "label": "What can you filter?",
              "value": "Add additional filters to vector store. You can also filter with flow config, including the current \"state\":\n- `$flow.sessionId`\n- `$flow.chatId`\n- `$flow.chatflowId`\n- `$flow.input`\n- `$flow.state`\n"
            },
            "id": "retrieverTool_0-input-retrieverToolMetadataFilter-json"
          }
        ],
        "inputAnchors": [
          {
            "label": "Retriever",
            "name": "retriever",
            "type": "BaseRetriever",
            "id": "retrieverTool_0-input-retriever-BaseRetriever"
          }
        ],
        "inputs": {
          "name": "search_for_property",
          "description": "Use this tool to handle questions about rental properties, including pricing, availability, location preferences, property features (e.g., pet-friendly, parking, proximity to schools), and short-term or long-term rental options. When interacting with users, always gather complete information, such as budget, type of property, number of rooms, and any specific requirements to provide accurate responses.\n\nIf you encounter a question you are not trained to answer or do not understand, respond with: 'I didnâ€™t understand what you asked. Would you like me to connect you to someone who can help?",
          "retriever": "",
          "returnSourceDocuments": true,
          "retrieverToolMetadataFilter": ""
        },
        "outputAnchors": [
          {
            "id": "retrieverTool_0-output-retrieverTool-RetrieverTool|DynamicTool|Tool|StructuredTool|Runnable",
            "name": "retrieverTool",
            "label": "RetrieverTool",
            "description": "Use a retriever as allowed tool for agent",
            "type": "RetrieverTool | DynamicTool | Tool | StructuredTool | Runnable"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 654,
      "selected": false,
      "positionAbsolute": {
        "x": 1299.9525798908048,
        "y": -768.4934911176385
      },
      "dragging": false
    },
    {
      "id": "pinecone_0",
      "position": {
        "x": 906.3934049289566,
        "y": -750.3962873399313
      },
      "type": "customNode",
      "data": {
        "id": "pinecone_0",
        "label": "Pinecone",
        "version": 5,
        "name": "pinecone",
        "type": "Pinecone",
        "baseClasses": [
          "Pinecone",
          "VectorStoreRetriever",
          "BaseRetriever"
        ],
        "category": "Vector Stores",
        "description": "Upsert embedded data and perform similarity or mmr search using Pinecone, a leading fully managed hosted vector database",
        "inputParams": [
          {
            "label": "Connect Credential",
            "name": "credential",
            "type": "credential",
            "credentialNames": [
              "pineconeApi"
            ],
            "id": "pinecone_0-input-credential-credential"
          },
          {
            "label": "Pinecone Index",
            "name": "pineconeIndex",
            "type": "string",
            "id": "pinecone_0-input-pineconeIndex-string"
          },
          {
            "label": "Pinecone Namespace",
            "name": "pineconeNamespace",
            "type": "string",
            "placeholder": "my-first-namespace",
            "additionalParams": true,
            "optional": true,
            "id": "pinecone_0-input-pineconeNamespace-string"
          },
          {
            "label": "File Upload",
            "name": "fileUpload",
            "description": "Allow file upload on the chat",
            "hint": {
              "label": "How to use",
              "value": "\n**File Upload**\n\nThis allows file upload on the chat. Uploaded files will be upserted on the fly to the vector store.\n\n**Note:**\n- You can only turn on file upload for one vector store at a time.\n- At least one Document Loader node should be connected to the document input.\n- Document Loader should be file types like PDF, DOCX, TXT, etc.\n\n**How it works**\n- Uploaded files will have the metadata updated with the chatId.\n- This will allow the file to be associated with the chatId.\n- When querying, metadata will be filtered by chatId to retrieve files associated with the chatId.\n"
            },
            "type": "boolean",
            "additionalParams": true,
            "optional": true,
            "id": "pinecone_0-input-fileUpload-boolean"
          },
          {
            "label": "Pinecone Text Key",
            "name": "pineconeTextKey",
            "description": "The key in the metadata for storing text. Default to `text`",
            "type": "string",
            "placeholder": "text",
            "additionalParams": true,
            "optional": true,
            "id": "pinecone_0-input-pineconeTextKey-string"
          },
          {
            "label": "Pinecone Metadata Filter",
            "name": "pineconeMetadataFilter",
            "type": "json",
            "optional": true,
            "additionalParams": true,
            "id": "pinecone_0-input-pineconeMetadataFilter-json"
          },
          {
            "label": "Top K",
            "name": "topK",
            "description": "Number of top results to fetch. Default to 4",
            "placeholder": "4",
            "type": "number",
            "additionalParams": true,
            "optional": true,
            "id": "pinecone_0-input-topK-number"
          },
          {
            "label": "Search Type",
            "name": "searchType",
            "type": "options",
            "default": "similarity",
            "options": [
              {
                "label": "Similarity",
                "name": "similarity"
              },
              {
                "label": "Max Marginal Relevance",
                "name": "mmr"
              }
            ],
            "additionalParams": true,
            "optional": true,
            "id": "pinecone_0-input-searchType-options"
          },
          {
            "label": "Fetch K (for MMR Search)",
            "name": "fetchK",
            "description": "Number of initial documents to fetch for MMR reranking. Default to 20. Used only when the search type is MMR",
            "placeholder": "20",
            "type": "number",
            "additionalParams": true,
            "optional": true,
            "id": "pinecone_0-input-fetchK-number"
          },
          {
            "label": "Lambda (for MMR Search)",
            "name": "lambda",
            "description": "Number between 0 and 1 that determines the degree of diversity among the results, where 0 corresponds to maximum diversity and 1 to minimum diversity. Used only when the search type is MMR",
            "placeholder": "0.5",
            "type": "number",
            "additionalParams": true,
            "optional": true,
            "id": "pinecone_0-input-lambda-number"
          }
        ],
        "inputAnchors": [
          {
            "label": "Document",
            "name": "document",
            "type": "Document",
            "list": true,
            "optional": true,
            "id": "pinecone_0-input-document-Document"
          },
          {
            "label": "Embeddings",
            "name": "embeddings",
            "type": "Embeddings",
            "id": "pinecone_0-input-embeddings-Embeddings"
          },
          {
            "label": "Record Manager",
            "name": "recordManager",
            "type": "RecordManager",
            "description": "Keep track of the record to prevent duplication",
            "optional": true,
            "id": "pinecone_0-input-recordManager-RecordManager"
          }
        ],
        "inputs": {
          "document": [
            "{{docxFile_0.data.instance}}"
          ],
          "embeddings": "{{ollamaEmbedding_0.data.instance}}",
          "recordManager": "",
          "pineconeIndex": "ollama-agent",
          "pineconeNamespace": "",
          "fileUpload": "",
          "pineconeTextKey": "",
          "pineconeMetadataFilter": "",
          "topK": "",
          "searchType": "similarity",
          "fetchK": "",
          "lambda": ""
        },
        "outputAnchors": [
          {
            "name": "output",
            "label": "Output",
            "type": "options",
            "description": "",
            "options": [
              {
                "id": "pinecone_0-output-retriever-Pinecone|VectorStoreRetriever|BaseRetriever",
                "name": "retriever",
                "label": "Pinecone Retriever",
                "description": "",
                "type": "Pinecone | VectorStoreRetriever | BaseRetriever"
              },
              {
                "id": "pinecone_0-output-vectorStore-Pinecone|VectorStore",
                "name": "vectorStore",
                "label": "Pinecone Vector Store",
                "description": "",
                "type": "Pinecone | VectorStore"
              }
            ],
            "default": "retriever"
          }
        ],
        "outputs": {
          "output": "retriever"
        },
        "selected": false
      },
      "width": 300,
      "height": 604,
      "selected": false,
      "positionAbsolute": {
        "x": 906.3934049289566,
        "y": -750.3962873399313
      },
      "dragging": false
    },
    {
      "id": "sqlDatabaseChain_0",
      "position": {
        "x": 482.2223554139824,
        "y": 42.888419238504554
      },
      "type": "customNode",
      "data": {
        "id": "sqlDatabaseChain_0",
        "label": "Sql Database Chain",
        "version": 5,
        "name": "sqlDatabaseChain",
        "type": "SqlDatabaseChain",
        "baseClasses": [
          "SqlDatabaseChain",
          "BaseChain",
          "Runnable"
        ],
        "category": "Chains",
        "description": "Answer questions over a SQL database",
        "inputParams": [
          {
            "label": "Database",
            "name": "database",
            "type": "options",
            "options": [
              {
                "label": "SQLite",
                "name": "sqlite"
              },
              {
                "label": "PostgreSQL",
                "name": "postgres"
              },
              {
                "label": "MSSQL",
                "name": "mssql"
              },
              {
                "label": "MySQL",
                "name": "mysql"
              }
            ],
            "default": "sqlite",
            "id": "sqlDatabaseChain_0-input-database-options"
          },
          {
            "label": "Connection string or file path (sqlite only)",
            "name": "url",
            "type": "string",
            "placeholder": "127.0.0.1:5432/chinook",
            "id": "sqlDatabaseChain_0-input-url-string"
          },
          {
            "label": "Include Tables",
            "name": "includesTables",
            "type": "string",
            "description": "Tables to include for queries, separated by comma. Can only use Include Tables or Ignore Tables",
            "placeholder": "table1, table2",
            "additionalParams": true,
            "optional": true,
            "id": "sqlDatabaseChain_0-input-includesTables-string"
          },
          {
            "label": "Ignore Tables",
            "name": "ignoreTables",
            "type": "string",
            "description": "Tables to ignore for queries, separated by comma. Can only use Ignore Tables or Include Tables",
            "placeholder": "table1, table2",
            "additionalParams": true,
            "optional": true,
            "id": "sqlDatabaseChain_0-input-ignoreTables-string"
          },
          {
            "label": "Sample table's rows info",
            "name": "sampleRowsInTableInfo",
            "type": "number",
            "description": "Number of sample row for tables to load for info.",
            "placeholder": "3",
            "additionalParams": true,
            "optional": true,
            "id": "sqlDatabaseChain_0-input-sampleRowsInTableInfo-number"
          },
          {
            "label": "Top Keys",
            "name": "topK",
            "type": "number",
            "description": "If you are querying for several rows of a table you can select the maximum number of results you want to get by using the \"top_k\" parameter (default is 10). This is useful for avoiding query results that exceed the prompt max length or consume tokens unnecessarily.",
            "placeholder": "10",
            "additionalParams": true,
            "optional": true,
            "id": "sqlDatabaseChain_0-input-topK-number"
          },
          {
            "label": "Custom Prompt",
            "name": "customPrompt",
            "type": "string",
            "description": "You can provide custom prompt to the chain. This will override the existing default prompt used. See <a target=\"_blank\" href=\"https://python.langchain.com/docs/integrations/tools/sqlite#customize-prompt\">guide</a>",
            "warning": "Prompt must include 3 input variables: {input}, {dialect}, {table_info}. You can refer to official guide from description above",
            "rows": 4,
            "placeholder": "Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. Unless the user specifies in his question a specific number of examples he wishes to obtain, always limit your query to at most {top_k} results. You can order the results by a relevant column to return the most interesting examples in the database.\n\nNever query for all the columns from a specific table, only ask for a the few relevant columns given the question.\n\nPay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n\nUse the following format:\n\nQuestion: \"Question here\"\nSQLQuery: \"SQL Query to run\"\nSQLResult: \"Result of the SQLQuery\"\nAnswer: \"Final answer here\"\n\nOnly use the tables listed below.\n\n{table_info}\n\nQuestion: {input}f-string",
            "additionalParams": true,
            "optional": true,
            "id": "sqlDatabaseChain_0-input-customPrompt-string"
          }
        ],
        "inputAnchors": [
          {
            "label": "Language Model",
            "name": "model",
            "type": "BaseLanguageModel",
            "id": "sqlDatabaseChain_0-input-model-BaseLanguageModel"
          },
          {
            "label": "Input Moderation",
            "description": "Detect text that could generate harmful output and prevent it from being sent to the language model",
            "name": "inputModeration",
            "type": "Moderation",
            "optional": true,
            "list": true,
            "id": "sqlDatabaseChain_0-input-inputModeration-Moderation"
          }
        ],
        "inputs": {
          "model": "{{ollama_0.data.instance}}",
          "database": "sqlite",
          "url": "",
          "includesTables": "",
          "ignoreTables": "",
          "sampleRowsInTableInfo": "",
          "topK": "",
          "customPrompt": "",
          "inputModeration": ""
        },
        "outputAnchors": [
          {
            "id": "sqlDatabaseChain_0-output-sqlDatabaseChain-SqlDatabaseChain|BaseChain|Runnable",
            "name": "sqlDatabaseChain",
            "label": "SqlDatabaseChain",
            "description": "Answer questions over a SQL database",
            "type": "SqlDatabaseChain | BaseChain | Runnable"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 527,
      "selected": false,
      "positionAbsolute": {
        "x": 482.2223554139824,
        "y": 42.888419238504554
      },
      "dragging": false
    },
    {
      "id": "chainTool_0",
      "position": {
        "x": 911.7365895724238,
        "y": -54.526376892654355
      },
      "type": "customNode",
      "data": {
        "id": "chainTool_0",
        "label": "Chain Tool",
        "version": 1,
        "name": "chainTool",
        "type": "ChainTool",
        "baseClasses": [
          "ChainTool",
          "DynamicTool",
          "Tool",
          "StructuredTool",
          "Runnable"
        ],
        "category": "Tools",
        "description": "Use a chain as allowed tool for agent",
        "inputParams": [
          {
            "label": "Chain Name",
            "name": "name",
            "type": "string",
            "placeholder": "state-of-union-qa",
            "id": "chainTool_0-input-name-string"
          },
          {
            "label": "Chain Description",
            "name": "description",
            "type": "string",
            "rows": 3,
            "placeholder": "State of the Union QA - useful for when you need to ask questions about the most recent state of the union address.",
            "id": "chainTool_0-input-description-string"
          },
          {
            "label": "Return Direct",
            "name": "returnDirect",
            "type": "boolean",
            "optional": true,
            "id": "chainTool_0-input-returnDirect-boolean"
          }
        ],
        "inputAnchors": [
          {
            "label": "Base Chain",
            "name": "baseChain",
            "type": "BaseChain",
            "id": "chainTool_0-input-baseChain-BaseChain"
          }
        ],
        "inputs": {
          "name": "",
          "description": "",
          "returnDirect": "",
          "baseChain": "{{sqlDatabaseChain_0.data.instance}}"
        },
        "outputAnchors": [
          {
            "id": "chainTool_0-output-chainTool-ChainTool|DynamicTool|Tool|StructuredTool|Runnable",
            "name": "chainTool",
            "label": "ChainTool",
            "description": "Use a chain as allowed tool for agent",
            "type": "ChainTool | DynamicTool | Tool | StructuredTool | Runnable"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 602,
      "selected": false,
      "positionAbsolute": {
        "x": 911.7365895724238,
        "y": -54.526376892654355
      },
      "dragging": false
    },
    {
      "id": "ollama_0",
      "position": {
        "x": 95.0021332805496,
        "y": 39.50146104490463
      },
      "type": "customNode",
      "data": {
        "id": "ollama_0",
        "label": "Ollama",
        "version": 2,
        "name": "ollama",
        "type": "Ollama",
        "baseClasses": [
          "Ollama",
          "LLM",
          "BaseLLM",
          "BaseLanguageModel",
          "Runnable"
        ],
        "category": "LLMs",
        "description": "Wrapper around open source large language models on Ollama",
        "inputParams": [
          {
            "label": "Base URL",
            "name": "baseUrl",
            "type": "string",
            "default": "http://localhost:11434",
            "id": "ollama_0-input-baseUrl-string"
          },
          {
            "label": "Model Name",
            "name": "modelName",
            "type": "string",
            "placeholder": "llama2",
            "id": "ollama_0-input-modelName-string"
          },
          {
            "label": "Temperature",
            "name": "temperature",
            "type": "number",
            "description": "The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 0.1,
            "default": 0.9,
            "optional": true,
            "id": "ollama_0-input-temperature-number"
          },
          {
            "label": "Top P",
            "name": "topP",
            "type": "number",
            "description": "Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "ollama_0-input-topP-number"
          },
          {
            "label": "Top K",
            "name": "topK",
            "type": "number",
            "description": "Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "ollama_0-input-topK-number"
          },
          {
            "label": "Mirostat",
            "name": "mirostat",
            "type": "number",
            "description": "Enable Mirostat sampling for controlling perplexity. (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "ollama_0-input-mirostat-number"
          },
          {
            "label": "Mirostat ETA",
            "name": "mirostatEta",
            "type": "number",
            "description": "Influences how quickly the algorithm responds to feedback from the generated text. A lower learning rate will result in slower adjustments, while a higher learning rate will make the algorithm more responsive. (Default: 0.1) Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "ollama_0-input-mirostatEta-number"
          },
          {
            "label": "Mirostat TAU",
            "name": "mirostatTau",
            "type": "number",
            "description": "Controls the balance between coherence and diversity of the output. A lower value will result in more focused and coherent text. (Default: 5.0) Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "ollama_0-input-mirostatTau-number"
          },
          {
            "label": "Context Window Size",
            "name": "numCtx",
            "type": "number",
            "description": "Sets the size of the context window used to generate the next token. (Default: 2048) Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "ollama_0-input-numCtx-number"
          },
          {
            "label": "Number of GQA groups",
            "name": "numGqa",
            "type": "number",
            "description": "The number of GQA groups in the transformer layer. Required for some models, for example it is 8 for llama2:70b. Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "ollama_0-input-numGqa-number"
          },
          {
            "label": "Number of GPU",
            "name": "numGpu",
            "type": "number",
            "description": "The number of layers to send to the GPU(s). On macOS it defaults to 1 to enable metal support, 0 to disable. Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "ollama_0-input-numGpu-number"
          },
          {
            "label": "Number of Thread",
            "name": "numThread",
            "type": "number",
            "description": "Sets the number of threads to use during computation. By default, Ollama will detect this for optimal performance. It is recommended to set this value to the number of physical CPU cores your system has (as opposed to the logical number of cores). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "ollama_0-input-numThread-number"
          },
          {
            "label": "Repeat Last N",
            "name": "repeatLastN",
            "type": "number",
            "description": "Sets how far back for the model to look back to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "ollama_0-input-repeatLastN-number"
          },
          {
            "label": "Repeat Penalty",
            "name": "repeatPenalty",
            "type": "number",
            "description": "Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "ollama_0-input-repeatPenalty-number"
          },
          {
            "label": "Stop Sequence",
            "name": "stop",
            "type": "string",
            "rows": 4,
            "placeholder": "AI assistant:",
            "description": "Sets the stop sequences to use. Use comma to seperate different sequences. Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "optional": true,
            "additionalParams": true,
            "id": "ollama_0-input-stop-string"
          },
          {
            "label": "Tail Free Sampling",
            "name": "tfsZ",
            "type": "number",
            "description": "Tail free sampling is used to reduce the impact of less probable tokens from the output. A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting. (Default: 1). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "ollama_0-input-tfsZ-number"
          }
        ],
        "inputAnchors": [
          {
            "label": "Cache",
            "name": "cache",
            "type": "BaseCache",
            "optional": true,
            "id": "ollama_0-input-cache-BaseCache"
          }
        ],
        "inputs": {
          "cache": "",
          "baseUrl": "http://localhost:11434",
          "modelName": "llama3.2",
          "temperature": 0.9,
          "topP": "",
          "topK": "",
          "mirostat": "",
          "mirostatEta": "",
          "mirostatTau": "",
          "numCtx": "",
          "numGqa": "",
          "numGpu": "",
          "numThread": "",
          "repeatLastN": "",
          "repeatPenalty": "",
          "stop": "",
          "tfsZ": ""
        },
        "outputAnchors": [
          {
            "id": "ollama_0-output-ollama-Ollama|LLM|BaseLLM|BaseLanguageModel|Runnable",
            "name": "ollama",
            "label": "Ollama",
            "description": "Wrapper around open source large language models on Ollama",
            "type": "Ollama | LLM | BaseLLM | BaseLanguageModel | Runnable"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 578,
      "selected": false,
      "positionAbsolute": {
        "x": 95.0021332805496,
        "y": 39.50146104490463
      },
      "dragging": false
    },
    {
      "id": "docxFile_0",
      "position": {
        "x": 476.291303706578,
        "y": -891.2612899601883
      },
      "type": "customNode",
      "data": {
        "id": "docxFile_0",
        "label": "Docx File",
        "version": 1,
        "name": "docxFile",
        "type": "Document",
        "baseClasses": [
          "Document"
        ],
        "category": "Document Loaders",
        "description": "Load data from DOCX files",
        "inputParams": [
          {
            "label": "Docx File",
            "name": "docxFile",
            "type": "file",
            "fileType": ".docx",
            "id": "docxFile_0-input-docxFile-file"
          },
          {
            "label": "Additional Metadata",
            "name": "metadata",
            "type": "json",
            "description": "Additional metadata to be added to the extracted documents",
            "optional": true,
            "additionalParams": true,
            "id": "docxFile_0-input-metadata-json"
          },
          {
            "label": "Omit Metadata Keys",
            "name": "omitMetadataKeys",
            "type": "string",
            "rows": 4,
            "description": "Each document loader comes with a default set of metadata keys that are extracted from the document. You can use this field to omit some of the default metadata keys. The value should be a list of keys, seperated by comma. Use * to omit all metadata keys execept the ones you specify in the Additional Metadata field",
            "placeholder": "key1, key2, key3.nestedKey1",
            "optional": true,
            "additionalParams": true,
            "id": "docxFile_0-input-omitMetadataKeys-string"
          }
        ],
        "inputAnchors": [
          {
            "label": "Text Splitter",
            "name": "textSplitter",
            "type": "TextSplitter",
            "optional": true,
            "id": "docxFile_0-input-textSplitter-TextSplitter"
          }
        ],
        "inputs": {
          "textSplitter": "{{recursiveCharacterTextSplitter_0.data.instance}}",
          "metadata": "",
          "omitMetadataKeys": ""
        },
        "outputAnchors": [
          {
            "id": "docxFile_0-output-docxFile-Document",
            "name": "docxFile",
            "label": "Document",
            "description": "Load data from DOCX files",
            "type": "Document"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 411,
      "selected": false,
      "positionAbsolute": {
        "x": 476.291303706578,
        "y": -891.2612899601883
      },
      "dragging": false
    },
    {
      "id": "conversationSummaryBufferMemory_0",
      "position": {
        "x": 905.0365481676045,
        "y": 1276.2756933325627
      },
      "type": "customNode",
      "data": {
        "id": "conversationSummaryBufferMemory_0",
        "label": "Conversation Summary Buffer Memory",
        "version": 1,
        "name": "conversationSummaryBufferMemory",
        "type": "ConversationSummaryBufferMemory",
        "baseClasses": [
          "ConversationSummaryBufferMemory",
          "BaseConversationSummaryMemory",
          "BaseChatMemory",
          "BaseMemory"
        ],
        "category": "Memory",
        "description": "Uses token length to decide when to summarize conversations",
        "inputParams": [
          {
            "label": "Max Token Limit",
            "name": "maxTokenLimit",
            "type": "number",
            "default": 2000,
            "description": "Summarize conversations once token limit is reached. Default to 2000",
            "id": "conversationSummaryBufferMemory_0-input-maxTokenLimit-number"
          },
          {
            "label": "Session Id",
            "name": "sessionId",
            "type": "string",
            "description": "If not specified, a random id will be used. Learn <a target=\"_blank\" href=\"https://docs.flowiseai.com/memory#ui-and-embedded-chat\">more</a>",
            "default": "",
            "optional": true,
            "additionalParams": true,
            "id": "conversationSummaryBufferMemory_0-input-sessionId-string"
          },
          {
            "label": "Memory Key",
            "name": "memoryKey",
            "type": "string",
            "default": "chat_history",
            "additionalParams": true,
            "id": "conversationSummaryBufferMemory_0-input-memoryKey-string"
          }
        ],
        "inputAnchors": [
          {
            "label": "Chat Model",
            "name": "model",
            "type": "BaseChatModel",
            "id": "conversationSummaryBufferMemory_0-input-model-BaseChatModel"
          }
        ],
        "inputs": {
          "model": "",
          "maxTokenLimit": 2000,
          "sessionId": "",
          "memoryKey": "chat_history"
        },
        "outputAnchors": [
          {
            "id": "conversationSummaryBufferMemory_0-output-conversationSummaryBufferMemory-ConversationSummaryBufferMemory|BaseConversationSummaryMemory|BaseChatMemory|BaseMemory",
            "name": "conversationSummaryBufferMemory",
            "label": "ConversationSummaryBufferMemory",
            "description": "Uses token length to decide when to summarize conversations",
            "type": "ConversationSummaryBufferMemory | BaseConversationSummaryMemory | BaseChatMemory | BaseMemory"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 380,
      "selected": false,
      "positionAbsolute": {
        "x": 905.0365481676045,
        "y": 1276.2756933325627
      },
      "dragging": false
    },
    {
      "id": "chatPromptTemplate_0",
      "position": {
        "x": 459.1951712946237,
        "y": 777.8131118635708
      },
      "type": "customNode",
      "data": {
        "id": "chatPromptTemplate_0",
        "label": "Chat Prompt Template",
        "version": 2,
        "name": "chatPromptTemplate",
        "type": "ChatPromptTemplate",
        "baseClasses": [
          "ChatPromptTemplate",
          "BaseChatPromptTemplate",
          "BasePromptTemplate",
          "Runnable"
        ],
        "category": "Prompts",
        "description": "Schema to represent a chat prompt",
        "inputParams": [
          {
            "label": "System Message",
            "name": "systemMessagePrompt",
            "type": "string",
            "rows": 4,
            "placeholder": "You are a helpful assistant that translates {input_language} to {output_language}.",
            "id": "chatPromptTemplate_0-input-systemMessagePrompt-string"
          },
          {
            "label": "Human Message",
            "name": "humanMessagePrompt",
            "description": "This prompt will be added at the end of the messages as human message",
            "type": "string",
            "rows": 4,
            "placeholder": "{text}",
            "id": "chatPromptTemplate_0-input-humanMessagePrompt-string"
          },
          {
            "label": "Format Prompt Values",
            "name": "promptValues",
            "type": "json",
            "optional": true,
            "acceptVariable": true,
            "list": true,
            "id": "chatPromptTemplate_0-input-promptValues-json"
          },
          {
            "label": "Messages History",
            "name": "messageHistory",
            "description": "Add messages after System Message. This is useful when you want to provide few shot examples",
            "type": "tabs",
            "tabIdentifier": "selectedMessagesTab",
            "additionalParams": true,
            "default": "messageHistoryCode",
            "tabs": [
              {
                "label": "Add Messages (Code)",
                "name": "messageHistoryCode",
                "type": "code",
                "hideCodeExecute": true,
                "codeExample": "const { AIMessage, HumanMessage, ToolMessage } = require('@langchain/core/messages');\n\nreturn [\n    new HumanMessage(\"What is 333382 ðŸ¦œ 1932?\"),\n    new AIMessage({\n        content: \"\",\n        tool_calls: [\n        {\n            id: \"12345\",\n            name: \"calulator\",\n            args: {\n                number1: 333382,\n                number2: 1932,\n                operation: \"divide\",\n            },\n        },\n        ],\n    }),\n    new ToolMessage({\n        tool_call_id: \"12345\",\n        content: \"The answer is 172.558.\",\n    }),\n    new AIMessage(\"The answer is 172.558.\"),\n]",
                "optional": true,
                "additionalParams": true
              }
            ],
            "id": "chatPromptTemplate_0-input-messageHistory-tabs"
          }
        ],
        "inputAnchors": [],
        "inputs": {
          "systemMessagePrompt": "",
          "humanMessagePrompt": "",
          "promptValues": "",
          "messageHistory": "messageHistoryCode"
        },
        "outputAnchors": [
          {
            "id": "chatPromptTemplate_0-output-chatPromptTemplate-ChatPromptTemplate|BaseChatPromptTemplate|BasePromptTemplate|Runnable",
            "name": "chatPromptTemplate",
            "label": "ChatPromptTemplate",
            "description": "Schema to represent a chat prompt",
            "type": "ChatPromptTemplate | BaseChatPromptTemplate | BasePromptTemplate | Runnable"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 740,
      "selected": false,
      "positionAbsolute": {
        "x": 459.1951712946237,
        "y": 777.8131118635708
      },
      "dragging": false
    },
    {
      "id": "autoGPT_0",
      "position": {
        "x": 1820.7723276554884,
        "y": -226.20284443802922
      },
      "type": "customNode",
      "data": {
        "id": "autoGPT_0",
        "label": "AutoGPT",
        "version": 2,
        "name": "autoGPT",
        "type": "AutoGPT",
        "baseClasses": [
          "AutoGPT"
        ],
        "category": "Agents",
        "description": "Autonomous agent with chain of thoughts for self-guided task completion",
        "inputParams": [
          {
            "label": "AutoGPT Name",
            "name": "aiName",
            "type": "string",
            "placeholder": "Tom",
            "optional": true,
            "id": "autoGPT_0-input-aiName-string"
          },
          {
            "label": "AutoGPT Role",
            "name": "aiRole",
            "type": "string",
            "placeholder": "Assistant",
            "optional": true,
            "id": "autoGPT_0-input-aiRole-string"
          },
          {
            "label": "Maximum Loop",
            "name": "maxLoop",
            "type": "number",
            "default": 5,
            "optional": true,
            "id": "autoGPT_0-input-maxLoop-number"
          }
        ],
        "inputAnchors": [
          {
            "label": "Allowed Tools",
            "name": "tools",
            "type": "Tool",
            "list": true,
            "id": "autoGPT_0-input-tools-Tool"
          },
          {
            "label": "Chat Model",
            "name": "model",
            "type": "BaseChatModel",
            "id": "autoGPT_0-input-model-BaseChatModel"
          },
          {
            "label": "Vector Store Retriever",
            "name": "vectorStoreRetriever",
            "type": "BaseRetriever",
            "id": "autoGPT_0-input-vectorStoreRetriever-BaseRetriever"
          },
          {
            "label": "Input Moderation",
            "description": "Detect text that could generate harmful output and prevent it from being sent to the language model",
            "name": "inputModeration",
            "type": "Moderation",
            "optional": true,
            "list": true,
            "id": "autoGPT_0-input-inputModeration-Moderation"
          }
        ],
        "inputs": {
          "tools": "",
          "model": "",
          "vectorStoreRetriever": "",
          "aiName": "",
          "aiRole": "",
          "maxLoop": 5,
          "inputModeration": ""
        },
        "outputAnchors": [
          {
            "id": "autoGPT_0-output-autoGPT-AutoGPT",
            "name": "autoGPT",
            "label": "AutoGPT",
            "description": "Autonomous agent with chain of thoughts for self-guided task completion",
            "type": "AutoGPT"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 678,
      "selected": false,
      "positionAbsolute": {
        "x": 1820.7723276554884,
        "y": -226.20284443802922
      },
      "dragging": false
    }
  ],
  "edges": [
    {
      "source": "ollamaEmbedding_0",
      "sourceHandle": "ollamaEmbedding_0-output-ollamaEmbedding-OllamaEmbeddings|Embeddings",
      "target": "pinecone_0",
      "targetHandle": "pinecone_0-input-embeddings-Embeddings",
      "type": "buttonedge",
      "id": "ollamaEmbedding_0-ollamaEmbedding_0-output-ollamaEmbedding-OllamaEmbeddings|Embeddings-pinecone_0-pinecone_0-input-embeddings-Embeddings"
    },
    {
      "source": "ollama_0",
      "sourceHandle": "ollama_0-output-ollama-Ollama|LLM|BaseLLM|BaseLanguageModel|Runnable",
      "target": "sqlDatabaseChain_0",
      "targetHandle": "sqlDatabaseChain_0-input-model-BaseLanguageModel",
      "type": "buttonedge",
      "id": "ollama_0-ollama_0-output-ollama-Ollama|LLM|BaseLLM|BaseLanguageModel|Runnable-sqlDatabaseChain_0-sqlDatabaseChain_0-input-model-BaseLanguageModel"
    },
    {
      "source": "sqlDatabaseChain_0",
      "sourceHandle": "sqlDatabaseChain_0-output-sqlDatabaseChain-SqlDatabaseChain|BaseChain|Runnable",
      "target": "chainTool_0",
      "targetHandle": "chainTool_0-input-baseChain-BaseChain",
      "type": "buttonedge",
      "id": "sqlDatabaseChain_0-sqlDatabaseChain_0-output-sqlDatabaseChain-SqlDatabaseChain|BaseChain|Runnable-chainTool_0-chainTool_0-input-baseChain-BaseChain"
    },
    {
      "source": "docxFile_0",
      "sourceHandle": "docxFile_0-output-docxFile-Document",
      "target": "pinecone_0",
      "targetHandle": "pinecone_0-input-document-Document",
      "type": "buttonedge",
      "id": "docxFile_0-docxFile_0-output-docxFile-Document-pinecone_0-pinecone_0-input-document-Document"
    },
    {
      "source": "recursiveCharacterTextSplitter_0",
      "sourceHandle": "recursiveCharacterTextSplitter_0-output-recursiveCharacterTextSplitter-RecursiveCharacterTextSplitter|TextSplitter|BaseDocumentTransformer|Runnable",
      "target": "docxFile_0",
      "targetHandle": "docxFile_0-input-textSplitter-TextSplitter",
      "type": "buttonedge",
      "id": "recursiveCharacterTextSplitter_0-recursiveCharacterTextSplitter_0-output-recursiveCharacterTextSplitter-RecursiveCharacterTextSplitter|TextSplitter|BaseDocumentTransformer|Runnable-docxFile_0-docxFile_0-input-textSplitter-TextSplitter"
    }
  ]
}